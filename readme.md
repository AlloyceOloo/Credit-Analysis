# CreditX – Credit Risk Analysis Pipeline

CreditX is a Python-based end-to-end credit analysis and risk modeling pipeline. It transforms raw customer financial data into engineered features, trains a predictive model, generates a scorecard for credit risk assessment, and supports inference for new customers. The project demonstrates expertise in data engineering, feature engineering, and applied machine learning for financial risk analysis.

---

## Table of Contents

1. [Project Overview](#project-overview)  
2. [Features](#features)  
3. [Installation](#installation)  
4. [Data](#data)  
5. [Project Structure](#project-structure)  
6. [Usage](#usage)  
    - [1. Data Ingestion](#1-data-ingestion)  
    - [2. Feature Engineering](#2-feature-engineering)  
    - [3. Model Training](#3-model-training)  
    - [4. Inference](#4-inference)  
7. [Testing](#testing)  
8. [Technology Stack](#technology-stack)  
9. [Notes](#notes)  
10. [Future Improvements](#future-improvements)  

---

## Project Overview

CreditX simulates a real-world credit analysis workflow. It starts with synthetic customer financial data, performs quality checks, computes engineered features, and trains a predictive LightGBM model. A logistic-based scorecard is generated for interpretability, allowing credit officers to evaluate risk systematically.

The pipeline emphasizes:

- **Data Quality:** Ensures all required features are present, validates ranges, and handles missing values.  
- **Feature Engineering:** Combines domain knowledge with numeric and categorical transformations, including debt-to-income ratios, utilization, and delinquency signals.  
- **Modeling:** LightGBM classifier for credit default prediction, evaluated with ROC-AUC and Gini metrics.  
- **Scorecard Creation:** Generates a simple, interpretable scoring system.  
- **Inference:** Predict new customer risk scores with reproducible, structured outputs.  

---

## Features

- Batch ingestion from CSV → Parquet  
- Data validation and QC metrics  
- Engineered numeric and categorical features:  
  - Age bands, income buckets  
  - Debt-to-income ratios, revolving balance ratios  
  - Delinquency indicators  
  - Normalized risk score  
- LightGBM binary classifier for credit risk  
- Scorecard for interpretability  
- ROC plots for model evaluation  
- Configurable pipeline with scripts and Makefile targets  

---

## Installation

1. Clone the repository:

```bash
git clone git@github.com:AlloyceOloo/Credit-Analysis.git
cd Credit-Analysis
```

2. Create a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

---

## Data

- **Synthetic Data:** Provided under `data/synthetic/customers.csv`  
- **Raw and Processed Data:** Saved to `data/raw/` and `data/processed/`  
- **Feature Data:** Generated by the pipeline in `data/features/`  

> ⚠️ Real financial datasets are sensitive. This project uses synthetic data to demonstrate functionality without exposing personal information.  

---

## Project Structure

```
CreditX/
│
├─ data/                 # Raw, processed, and feature data
│  ├─ synthetic/
│  ├─ raw/
│  ├─ processed/
│  └─ features/
│
├─ src/
│  ├─ ingestion/         # Batch ingest and data validation
│  ├─ features/          # Feature engineering
│  ├─ models/            # Model training and scorecard
│  └─ inference/         # Predictive inference scripts
│
├─ scripts/              # CLI wrappers for running pipeline stages
├─ tests/                # Unit tests
├─ artifacts/            # Model metrics, ROC plots, predictions
├─ models/               # Trained LightGBM and scorecard artifacts
├─ Makefile              # Preconfigured pipeline commands
└─ requirements.txt      # Python dependencies
```

---

## Usage

### 1. Data Ingestion

```bash
make ingest-data
```

- Reads synthetic CSV data  
- Runs basic data quality checks  
- Saves raw and processed Parquet files  
- Generates a `.meta.json` summary  

---

### 2. Feature Engineering

```bash
make build-features
```

- Builds engineered numeric and categorical features  
- Saves features to Parquet for modeling  

---

### 3. Model Training

```bash
make train-model
```

- Trains LightGBM classifier on engineered features  
- Generates ROC plot and evaluation metrics  
- Saves model and interpretable scorecard artifacts  

---

### 4. Inference

```bash
make predict
```

- Reads new customer data  
- Uses trained model and scorecard to generate predicted probabilities and scores  
- Saves predictions to `artifacts/predictions.csv`  

---

## Testing

Run unit tests:

```bash
make run-tests
```

- Ensures ingestion, feature engineering, and scorecard generation work correctly  

---

## Technology Stack

- **Python 3.10**  
- **Pandas, NumPy** – Data manipulation  
- **Scikit-learn** – Feature preprocessing and validation  
- **LightGBM** – Gradient boosting classifier  
- **Joblib** – Model and scorecard serialization  
- **Matplotlib** – ROC curve visualization  
- **Pytest** – Unit testing  

---

## Notes

- The project is designed for **reproducibility**: all pipeline steps are automated via scripts and Makefile commands.  
- The scorecard provides **interpretability** for decision-making in credit risk, complementing the predictive LightGBM model.  

---

## Future Improvements

- Integrate real-time scoring API  
- Include additional feature transformations (e.g., time-based aggregations)  
- Implement Streamlit dashboard for interactive exploration  
- Extend support for real-world datasets with missing or anomalous data  

---

**Author:** Alloyce Oloo

